{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y bitsandbytes\n",
    "!pip install bitsandbytes==0.41.1  # Replace with the latest version if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "dataset = pd.read_csv('/content/Poetic_Devices_Final.csv')\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "# Step 1: Define the model path\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Replace with your Hugging Face model path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configure and Load the Model with Int8 Quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable int8 quantization\n",
    "    bnb_8bit_use_double_quant=True,  # Better accuracy\n",
    "    bnb_8bit_quant_type=\"nf4\",  # Quantization type\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",  # Automatically map layers to devices\n",
    "    quantization_config=quantization_config,  # Apply int8 quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply LoRA for Trainable Adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target LoRA layers\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load and Preprocess Dataset\n",
    "df = pd.read_csv('/content/Poetic_Devices_Final.csv')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df = df[df['Text'].str.strip() != '']  # Remove empty strings\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Text\"], truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"Text\"],  # Replace \"Text\" with the correct column name from your dataset\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,  # Adjust max_length as needed\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for causal LM\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset (repeat this step to ensure labels are properly set)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"huggingface\", name=\"Thousand_epoch_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJTHnLwt747h"
   },
   "source": [
    "## BEFORE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Function to check model output and log to wandb\n",
    "def test_model_output(model, tokenizer, input_prompt, stage=\"Before Training\"):\n",
    "    # Tokenize the input prompt\n",
    "    tokenized_input = tokenizer(\n",
    "        input_prompt,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,  # Ensure it matches training settings\n",
    "    )\n",
    "\n",
    "    # Get model outputs\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "\n",
    "    # Decode the output for verification\n",
    "    decoded_output = tokenizer.decode(\n",
    "        torch.argmax(outputs.logits, dim=-1).squeeze().tolist(),\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Log input and output to wandb\n",
    "    wandb.log({\n",
    "        f\"{stage} Input Prompt\": input_prompt,\n",
    "        f\"{stage} Generated Output\": decoded_output\n",
    "    })\n",
    "\n",
    "    print(\"Input Prompt:\")\n",
    "    print(input_prompt)\n",
    "    print(\"\\nGenerated Output:\")\n",
    "    print(decoded_output)\n",
    "\n",
    "sample_prompt = \"Dani, a dangerous dragon, decided to decimate the dwelling, dousing it with devilish daring and gasoline. What are the poetic devices you can find from this line and also explain their interplay? How do these devices evoke emotion and meaning?\"\n",
    "test_model_output(model, tokenizer, sample_prompt, stage=\"Before Training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"my-model-training\", name=\"After Training\")\n",
    "\n",
    "# Create a callback to log metrics\n",
    "class MetricLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Collect metrics\n",
    "            metrics = {\n",
    "                \"epoch\": state.epoch,\n",
    "                \"training_loss\": logs.get(\"loss\"),\n",
    "                \"eval_loss\": logs.get(\"eval_loss\")\n",
    "            }\n",
    "            self.metrics.append(metrics)\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log(metrics)\n",
    "\n",
    "# Define training arguments with wandb integration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Training\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    dataloader_num_workers=0,\n",
    "    max_steps=1000,\n",
    "    report_to=\"wandb\"  # Enable wandb logging\n",
    ")\n",
    "\n",
    "# Instantiate the callback\n",
    "metric_logger = MetricLogger()\n",
    "\n",
    "# Define the Trainer with wandb logging\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[metric_logger]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "print(\"\\nSaving fine-tuned model...\")\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Generate Response AFTER Training\n",
    "\n",
    "print(\"\\nGenerating response after training...\")\n",
    "prompt = (\"Dani, a dangerous dragon, decided to decimate the dwelling, dousing it with devilish daring and gasoline. What are the poetic devices you can find from this line and also explain their interplay? How do these devices evoke emotion and meaning?\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reduce memory-intensive parameters\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=200,  # Shortened max length\n",
    "    num_beams=3,  # Fewer beams\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nResponse after training:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpcYqT7yqWnK"
   },
   "source": [
    "**Dataset Insights**\n",
    "\n",
    "This block creates a bar chart for analyzing the frequency of categories or themes in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Category' and 'Poetic Devices' columns exist in the original dataset\n",
    "category_counts = df['Category'].value_counts()\n",
    "\n",
    "# Plot category frequencies\n",
    "category_counts.plot(kind='bar', figsize=(10, 6), alpha=0.7, edgecolor='black')\n",
    "plt.title('Category Frequencies in Dataset')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Repeat for 'Poetic Devices'\n",
    "device_counts = df['Poetic Devices'].value_counts()\n",
    "device_counts.plot(kind='bar', figsize=(10, 6), alpha=0.7, edgecolor='black')\n",
    "plt.title('Poetic Devices Frequencies in Dataset')\n",
    "plt.xlabel('Poetic Device')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXgoBvC6qhiU"
   },
   "source": [
    "**Training Metrics**\n",
    "\n",
    "This block visualizes training and validation loss across epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a prompt and tokenize it\n",
    "prompt = \"Dreams fell like autumn leaves, scattered in the winds of despair.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Forward pass with attention outputs enabled\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Extract attention weights (last layer)\n",
    "attention_weights = outputs.attentions[-1].squeeze(0).detach().cpu().numpy()  # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "# Plot attention weights for a single head (choose the first head for simplicity)\n",
    "head_attention = attention_weights[0]\n",
    "\n",
    "# Create a heatmap for the attention weights\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(head_attention, cmap='viridis', annot=False)\n",
    "plt.title('Attention Heatmap (First Head)')\n",
    "plt.xlabel('Input Tokens')\n",
    "plt.ylabel('Output Tokens')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EVL-hp_7VBu"
   },
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/content/Poetic_Devices_Final.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Load a zero-shot classification model with GPU enabled\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "\n",
    "# Define the target emotions\n",
    "target_emotions = [\"love\", \"sorrow\", \"anger\", \"hatred\"]\n",
    "\n",
    "# Function to classify a batch of texts\n",
    "def classify_emotion_batch(texts, threshold=0.5):\n",
    "    results = classifier(texts, candidate_labels=target_emotions, multi_label=False)\n",
    "    emotions = []\n",
    "    confidences = []\n",
    "\n",
    "    for res in results:\n",
    "        # Check the top prediction\n",
    "        top_label = res['labels'][0]\n",
    "        top_score = res['scores'][0]\n",
    "\n",
    "        # Assign \"neutral\" if confidence is below the threshold\n",
    "        if top_score < threshold:\n",
    "            emotions.append(\"neutral\")\n",
    "            confidences.append(top_score)\n",
    "        else:\n",
    "            emotions.append(top_label)\n",
    "            confidences.append(top_score)\n",
    "\n",
    "    return emotions, confidences\n",
    "\n",
    "# Process the data in batches\n",
    "batch_size = 16\n",
    "emotions = []\n",
    "confidences = []\n",
    "\n",
    "for i in range(0, len(data), batch_size):\n",
    "    batch_texts = data['Text'][i:i + batch_size].tolist()\n",
    "    batch_emotions, batch_confidences = classify_emotion_batch(batch_texts)\n",
    "    emotions.extend(batch_emotions)\n",
    "    confidences.extend(batch_confidences)\n",
    "\n",
    "# Add results to the dataframe\n",
    "data['Emotion'] = emotions\n",
    "data['Confidence'] = confidences\n",
    "\n",
    "# Save the results to a new file\n",
    "output_path = '/content/Zero_Shot_Emotion_Annotated_Neutral.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "# Preview the results\n",
    "print(data.head())\n",
    "print(f\"Annotated dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
